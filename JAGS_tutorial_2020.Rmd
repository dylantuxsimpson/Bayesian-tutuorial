---
title: "Bayesian tutorial"
output: html_document
---

```{r setup, include = F}
library(lme4)
library(R2jags)
library(tidyverse)
library(MASS)
# The following are functions from John Kruschke's book Doing Bayesian Data Analysis 
# that are really helpful for visualizing results from Bayesian models:
source('bayesFunctions.R')
```
# JAGS, an introduction
Before we get into how to code and run our models, let's briefly consider the alogrithm and software we are running. The software is JAGS (Just Another Gibbs Sampler), which compiles a number of algorithms to estimate Bayesian posteriors using Markov Chain Monte Carlo (MCMC) sampling.

MCMC samplers estimate marginal posterior probability distributions by effectively *sampling* values from those distributions. That sounds like magic, but but it's actually just very clever. When we formulate our models, we say that the posterior distribution is proportional to the distribution described by the product of the likelihood and the priors. What MCMC does is samples possible values of each parameter with a frequency governed by the relative likelihood of that value, given the distribution described by that likelihood*prior product. So values of a parameter with a high relative likelihood get sampled frequently, and values with a low relative likelihood get sampled infrequently. It does this in a manner conceptually similar to numeric maximum likelihood estimators: drunkenly hiking around parameter space. Except whereas in MLE we really just want the peak of the mountain, in MCMC we want a topography map of the whole mountain. In the end, JAGS gives us a collection of sampled parameter values. Imagine the hikers have GPS that ping coordinates and altitude every few seconds as they explore the mountain. We describe the posterior distributions by looking at the empirical distributions of these GPS pings. This is called a Monte Carlo process because we are describing probability distribuions using empircal samples, and it's a Marcov Chain process because the algorithm moves around parameter space in sequence (like a hiker). As a result, each value in the chain depends on the previous sampled value (like a hiker).

Effectively, how JAGS operates is to 1) start with either a given or random starting value for each parameter. 2) Calculate the likelihood of the model given these parameters, 3) sample a new value for one parameter, while holding all others constant; the value of this parameter is governed by the relative likelihoods of new values, given the current values of all other parameters. 4) Move to the next parameter and choose a new value as in (3). 5) Repeat (4) for each paramter; this constitutes one step, or iteration, of the MCMC algorithm. 6) Repeat (2-5) for something like 10,000 or more iterations. In practice, we will discard the first few thousand iterations, because we only want to estimate the posterior based on the samples taken once the model has 'converged' on the correct area of parameter space. After this 'burn in' period, we keep track of the values in each step. These values form an empirical estimation of the posterior distribution.

Another general practice is to run the MCMC algorithm multiple times, in parallel. These parallel runs are called "chains," and offer some insurance against the possibility that the MCMC algorithm gets stuck at a local maximum in probability space (imagine a bimodal distribution - we want to describe the whole distribution, but the algorithm might get stuck in one of those peaks).

Okay, with that, let's write some models in JAGS.

# Simple linear model

Let's start with a simple linear model. Because it can be illuminating to see how well our statistical methods can capture things we know to be true, we'll simulate some data.

Let's imagine we measure some continuous variable and two things we expect to drive it. Here, I say the true population mean is 15, then use rnorm() to generate the predictor variables according to standard-normal distribution (i.e. mean 0, sd 1). This is usually good practice to do this with your own data by centering and scaling (subtracting the mean from each datum and dividing by the standard deviation). There are couple reasons for this. First, it makes effect sizes comparable among your predictor variables (by making effect size relative to how much that thing actually varies; i.e. effect per standard deviation). Second, there are big computational benefits. The drunken hikers are way better at finding the peak when variables are measured on the same scale because they have the same distance to cover in each direction.

I also assign a standard deviation of 4 to the model.

Lastly, I run a frequentist linear model that we can use to compare to our Bayesian model.

```{r Basic lm sim}
# Total number of observations:
N <- 100
# True parameter values:
b0 <- 15
b1 <- 3
# Observed predictors:
x <- rnorm(N)

# Standard deviation of the model
s <- 4

# Observations:
y <- rnorm(N, mean = b0 + b1*x, sd = s)

# Model:
fm <-lm(y ~ x)
```

Next, before we look at the code to run this model in JAGS, let's look at the probability statement for this model. The probability statement is the mathematical description of the model, and is the product of the model likelihood (i.e. probability of the data given the model) and the probability of the model (i.e. the priors). 

We have to do this because coding a model in JAGS is a direct (or maybe indirect) transcription of the model probability statement. The great but also terrible thing about JAGS is that you code all your models from scratch. Which means we get to be explicit about how every variable relates to every other, and how every variable is distributed. On the other hand, it means we have to be explicit about how every variable relates to every other, and how every variable is distributed. All of this is contained in the model probability statement. 

Before diving in, a note on notation: the model above is often described like this:
$$ y_i = \beta_0 + \beta_1 x_i + e_i \\ e_i \sim norm(0, \sigma^2)$$
but can also be described like this: 

$$y_i \sim norm(\beta_0 + \beta_1 x_i,\  \sigma^2)$$
This means the same thing, but is more explicit about the distribution of $\mathbf{y}$ as a whole. That is, that $y_i$ is a realization of a normal process with variance $\sigma^2$ and a mean that is governed by the linear model $\beta_0 + \beta_1 x_i$. 

From here, we want to describe the model likelihood, which is the probability of the data, given the model. To do this, we'll modify the second notation from above, and describe the probability of $y_i$ as
$$ norm(y_i | \beta_0 + \beta_1 x_i,\  \sigma^2)$$
Thus, the model likelihood would be the product of this expression across all values of $i$.

So, on to the full model probability statement. The posterior probability of the model is proportional to the product of the model likelihood and the prior model probability. Here, I assign vague priors to each parameter:

$$Pr(\beta_0, \beta_1, \sigma | y) \propto \Pi_{i=1}^N norm(y_i | \beta_0 + \beta_1 x_i,\  \sigma^2) \times norm(\beta_0|0, 10) \times norm(\beta_1|0,10) gamma(\sigma|1,0.01)$$

Okay, so how do we give this model to JAGS? Broadly, you may see this done elsewhere in a few different ways, often by saving as and then calling it back from a text file. By using the R2jags package we can save it as a "function" within the R environment. I like this method because you get all the syntax help from RSTudio (e.g. matching parentheses). If you write it as a text string (i.e. within quotation marks), it all shows up as the same color and RStudio doesn't pay attention to syntax.

More specifically, we are going to write a script that translates the above probability statement into something that looks like R code but isn't quite. It might be easiest just to demonstrate but, first, some things to note about JAGS syntax:

* The for() loop is not the same as in R. It is not iterating an operation, but instead making repeated "declarations" (JAGS is considered a 'declarative' programming language). Remember that the Bayesian model is essentially a long product of probability expressions. The likelihood in a Bayesian model is the product of the probabilities of each datum given the model. In mathematical notation, we use a $\Pi$ symbol to denote the product across a series of expressions. You can think of the for() loop in JAGS as the product symbol. The for() loop expands this product to give JAGS an explicit delcaration for each datum. That is, it will explicitly declare a probability statement for each observation. After the for() loop, we include the expressions for the prior probabilities. We do not include the priors within the loop because we do not need to declare an independent prior for each observation, we only declare them once.

* Probability distributions are declared using their density function, e.g. dnorm(). The arguments to these functions are the parameters to that distributions. A notable except is that, when you specify a normal distribution, you do not supply standard deviation (as you do in R) or varaince, but *precision*. Precision is the inverse of variance (i.e. $1/\sigma^2$). I can speculate as to why this is, but this is not the place. Just know it's a thing, and that when I say dnorm(0,0.01), I mean a normal distribution with mean = 0 and variance = 100.

* Objects in JAGS are all considered to be arrays, which are n-dimensional matrices. A single value is a 1x1 array, a vector of 10 values is a 1x10 array, a 4x4 matrix is a 4x4 array, etc. Variables are all indexed in JAGS the same way they are in R, e.g. x[i,j]. The dimensions of an array are intuited by JAGS based on the maximum number of dimensions you give it. If you only ever refer to beta, without indexing, JAGS knows it is a single value; if you refer to y[i], then JAGS knows it's a vector; if you refer to X[i,j], JAGS knows it's a matrix. Etc. If, however, you once refer to X[i] and later to X[i,j], JAGS will get confused.

* Random variables/probabilistic relationships are denoted using the tilde ($\sim$), while fixed values/deterministic relationships are denoted using the assignment operator (<-). So, in a JAGS model, you can say that $y_i$ is a realization of a normal process with mean $\mu_i$ and variance $\sigma^2$ by writing y[i] ~ norm(mu[i], sigma^2), and you can say that each $\mu_i$ is defined by the linear model $\beta_0 + \beta_1 x_i$ by writing mu[i] <- beta0 + beta1*x[i]. In each of these cases, we are indexing variables by observations, i, using brackets.

Just as important, something to remember about about priors:

Even a vague prior contains information. These are often called "uninformative" priors, but this is arguably a misnomer; even if they are non-specific or imprecise, a prior inherently carries with it an assumption over the sort of values a parameter can take and, therefore, its underlying generative process. This can cause problems if you make impossible assumptions, like providing a normal prior for a variance term (this assumes variance can be negative). It can also cause problems if you make silly assumptions. For instance, it is common to put uniform priors on variance parameters (e.g. $\sigma \sim unif(0,100)$). This assumes that there is equal probability that your standard deviation is 0.001 and 100. If your parameter space (governed by your range of observed values $\mathbf{y}$) is limited to values between -10 and 10, how probable is a variance of 10,000? Putting so much probability mass where it obviously doesn't belong can affect your posteriors, especially with small sample sizes.

Okay, here's the model. I used the same priors I described above. The normal priors suggest that betas could be any real number, but are most likely to be 0, though we wouldn't be surprised if they were any value in [-20, 20]. The gamma prior suggests variance could take on a broad range of values but is more likely to be small than large.
```{r JAGS linear model}
lm.mod <- function(){
  # Likelihood:
  # (remember that the for() loops stands in for the product symbol,
  # It is going to declare the statements within the loop for each value of i.)
  for(i in 1:N){
    # each y_i is a realization of a normal distribution with mean mu_i and variance sig.sq
    y[i] ~ dnorm(mu[i], 1/sig.sq) 
    # mu_i is defined deterministically with a linear model.
    # (we could have simply written out the linear model within dnrom() above, but this
    # reads more clearly to me.)
    mu[i] <- b0 + b1*x[i]
  } # we end the loop because we are done with variables indexed by i
  
  # Priors:
  b0 ~ dnorm(0, .01) # precision .01 = variance 100
  b1 ~ dnorm(0, .01)
  
  sig.sq ~ dgamma(1, .01)
  # Because we simulated the data with standard deviation, that's what we want to look at:
  sigma <- sqrt(sig.sq) # All this is doing is giving us a transformed variable for convenience.
                        # It isn't interacting with the rest of the model.
}
```


To run the model in JAGS, we need to create a named list of all our variables, and provide initial values for the algorithm (i.e., give our partying hikers a trailhead). 

We need to supply initial values for every variable that gets a prior. That is, for every variable that isn't conditioned on another variable in the model (in a Bayesian network, this is every variable that does not have an arrow point to it). Remember that we will run the model multiple times in parallel, each run called a 'chain.' (Each chain is like its own hiker exploring Probaiblity Mountain.) It is good practice to provide unique inital values for each chain (i.e. give each hiker their own trail head). The intial value, by default, is the mean of the prior, which would mean the default is for each chain to have the same initial value. To give them independent values, I instead take random draws from their priors.

Initial values need to be provided as a list of named lists, which is kind of annoying. I show a couple ways of doing it. The first is to write out each sublist independently. This is good if you are providing explicit and intentional value for each chain. This might actually be good practice so that you can make sure your initial values are dispersed well across your prior. However, I am not convinced this is of utmost importance, given a solid burn-in period, and I am lazy, so I typically just the use the replicate command with rdist() functions.
```{r JAGS set up lm}
# Here we take all the data from our R environment that appear in our JAGS model and save them into a list.
# Each element in the list is named for how it appears in the JAGS model.
dataList.lm <- list(
  y = y, # the observations
  N = length(y), # the number of observations (used to parameterize the for() loops)
  x = x # predictor data
)

# Next we create a list of intitial values. Each chain gets its own list of values, 
# each named for the variable in the JAGS model.
# We then compile each chain's list into a list of lists.

# The clearer but longer way of doing initial values:
# initial.values.lm <- list(
#   list(alpha = rnorm(1, 0, 30), b1 = rnorm(1, 0, 30), b2 = rnorm(1, 0, 30), tau = rgamma(1, .001, .001)),
#   list(alpha = rnorm(1, 0, 30), b1 = rnorm(1, 0, 30), b2 = rnorm(1, 0, 30), tau = rgamma(1, .001, .001)),
#   list(alpha = rnorm(1, 0, 30), b1 = rnorm(1, 0, 30), b2 = rnorm(1, 0, 30), tau = rgamma(1, .001, .001))
# )

# We can also to do the same thing with less text (but in a non-intuitive way...):
# This is nice because it takes less space, 
# and bc you can change the number of chains just by changing the first argument of replicate()

initial.values.lm <- replicate(3, list(list(
  b0 = rnorm(1, 0, 30), b1 = rnorm(1, 0, 30), sig.sq = rgamma(1, 1, .1)
  )))
```

Now we run the model. The jags() function from the R2jags package needs a model, a data list, and a vector with the names of parameters you will want posteiors for. Adding parameters will not add computation time, but will add to the size of the object you save. (That is, it will take posterior draws for every variable in the model regardless, this is just saying which ones it records.) The rest have defaults, but I provide some values for the ones I think matter:

* The number of iterations (n.iter) is the total number of draws from the posterior distribution that JAGS will take.
* The burn-in period (n.burnin) is the number of draws that are discarded before JAGS starts recording. If you think of the initial values as the random places across the park where the drunken hikers wake up after their black out, the burn-in period is the time during which they find their trail head. Once they get to the right mountain, they will crawl all around it to describe its topography, and this is what we want to record. The burn-in period is them finding their way there from the parking lot, or the pit toilet, or where ever, and we don't care about the topography of the parking lot.
* The number of chains is the number of drunken hikers. With more hikers, it takes less time to explore the mountain. This is especially helpful if you are using parall processing, because each hiker (chain) can be operated by a separate processor core. It is considered good practice to have at least three chains.
* Thinning (n.thin) is a process in which we only save one in so many draws. This is to reduce autocorrelation in the posterior draws. Here, with a simple model, saving one in three is sufficient. Sometimes, in complex models especially, you will need to save fewer, and therefore run more total iterations to maintain sample size. You can asess this heuristically by looking at traceplots (see below) or quantitatively by calculating 'effective sample size,' which adjusts for autocorrelation.

At the bottom, I convert the model object into a couple useful forms. The MCMC object is helpful for visualizing each chain indpendently to assess convergence (did all the hikers cover the whole mountain?), whereas the dataframe is more useful for plotting the posterior distributions.

```{r Run JAGS lm}
fm.jags.lm <- jags(
  model.file = lm.mod,
  data = dataList.lm,
  inits = initial.values.lm,
  parameters.to.save = c('b0', 'b1', 'sigma'),
  n.iter = 20000,
  n.burnin = 5000,
  n.chains = 3,
  n.thin = 3
)
fm.lm.mcmc <- as.mcmc(fm.jags.lm)
fm.lm.dat <- as.data.frame(as.matrix(fm.lm.mcmc))
```

Here we check diagnostics for the model. 

First we create a traceplot of each parameter. A traceplot shows each posterior draw in sequence, with the chains laid atop one another. That is, it is a line plot in which the x-axis is the index number of each iteration of the MCMC process in sequence, and the y-axis is the value of the parameter at each draw. Each chain is given its own color, and the lines are drawn on top of one another. We can use these to visually assess "convergence," which is whether the chains have all arrived in the same region of parameter space. That is, that the drunken hikers have found each other, and are now running around the peak together, rather than wandering lost and alone. The primary thing we are looking for is that the traceplots all overlap - not perfectly, they are independent, stochastic processes. But that they zig zag up and down across the same general range of values. The second thing we want to see is that the lines travel up and down freely, rather than slowly arcing. This can be a sign that there is really high autocorrelation between samples. There is always some, which is why we thin, but in complex models the algorithm can have trouble moving around parameter space - essentially, the hikers to too small of steps, and move too slowly around the landscape. When this is the case, we essentially have pseudoreplication among our posterior draws.

Second, we calculate the Gelman diagnostic, which is a quantitative measure of convergence. It is based on a comparison on within- to between-chain variance, analogous to an ANOVA (i.e. are the chains different?). Ideally, the diagnostic value is 1.0. General rule of thumb is <1.1 is okay.

In each of these, our diagnostics look great. All the of the chains are plotted atop one another, and our Gelman diagnostics all equal 1. More complex models often don't look as good.
```{r lm diagnostics}
traceplot(fm.lm.mcmc)
gelman.diag(fm.lm.mcmc)
```

Next are the resulting marginal posterior distributions. 

Looking at the plots, notice Kruschke's function gives the mode, rather than the mean. This is because sometimes posteriors are skewed, and we typically care about where the bulk of probability mass lies, so we use the mode because the mean is influenced by long tails. In a truly normal distribution, these are the same. The posteriors for variance parameters, though, often come out right-skewed, even when they are given uniform priors. These plots also show the 'highest density interval,' which is the smallest range of values to contain 95% of the probaiblity mass. It is an empirical method of getting 95% CIs, and is slightly different than quantiles. Don't worry about the distinction for now. I am not sure there is a concensus on which is better, and they are typically much the same.

Kruschke's function also outputs effective sample size (ESS) for each parameter. Because MCMC is an autocorrelative process, sometimes your posterior draws are not sufficiently independent. ESS measures autocorrelation in your posterior draws and gives an adjusted measure of your sample size. Here, our model is simple and JAGS had no problem. ESS is nearly equal to the 15000 draws we saved.

```{r lm results}
plotPost(fm.lm.dat$b0, main = "intercept")
plotPost(fm.lm.dat$b1, main = 'beta 1')
plotPost(fm.lm.dat$sig, main = 'sd')
summary(fm)
```


# A generalized linear model

Here we'll expand what we just did to a generalized linear model, to show how we can include link functions. These are real data on the number of crab burrows as a function of salinity. This is count data, so it could be modeled with a Poisson or negative-binomial. For simplicity, here, we won't worry about which is better and just use a Poisson. In Poisson regression, we use the log link:
$$ Poisson(y_i|\lambda_i) \\
  \lambda_i = e^{\beta_0 + \beta_1 x_i}$$
The probability statement for this model looks like this:
$$Pr(\beta_0, \beta_1 | y) \propto \Pi_{i=1}^N Pois(y_i | e^{\beta_0 + \beta_1 x_i}) \times norm(\beta_0|0, 10) \times norm(\beta_1|0,10)$$

```{r Try it - data}
crabs <- read.csv('crab_burrows.csv')
head(crabs)
```

Below, we'll translate this probability statement into a JAGs model. 

```{r Try it - model}
crab.mod <- function(){
  # Declare the process underlying each observation:
  for(i in 1:N){
    ## Likelihood:
    # Each observation is a realization of a Poisson process w/ intensity lambda
    
    # The lambda for each observation is related to the linear model
    # via the log link function:
    
  }
  ## Priors:
  
}
```

Here, set up your data list and initial values.
```{r Try it - set up}
datList.crabs <- list(

)

crab.inits <- replicate(3, list(list(
  b0 = rnorm(1, 0, 10), b1 = rnorm(1, 0, 10)
)))
```

```{r Try it - run model}
fm.crabs <- jags(
  data = datList.crabs,
  model.file = crab.mod,
  parameters.to.save = c('b0', 'b1'),
  inits = crab.inits,
  n.chains = 3,
  n.iter = 20000,
  n.burnin = 5000,
  n.thin = 3
)

fm.crabs.mcmc <- as.mcmc(fm.crabs) # mcmc object is a special list for JAGS related functions/packages
fm.crabs.dat <- as.data.frame(as.matrix(fm.crabs.mcmc)) # here, each column is a parameter, 
                                                        # each row is an iteration
```

```{r Crab model - diagnostics}
traceplot(fm.crabs.mcmc)
gelman.diag(fm.crabs.mcmc)
```

```{r Crab model - results}
plotPost(fm.crabs.dat$b0, main = "ln(mean abundance)")
plotPost(fm.crabs.dat$b1, main = "Change in ln(lambda) per unit change in salinity")
```
